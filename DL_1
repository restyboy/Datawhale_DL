#【任务1 - 线性回归算法梳理】
##机器学习的一些概念
###有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
1. *监督式学习*：在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）
2. *无监督式学习*：在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。
3. *泛化能力*：指机器学习算法对新鲜样本的适应能力,，简而言之是在原有的数据集上添加新的数据集，通过训练输出一个合理的结果。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。
4. *过拟合*：对当前已有的数据集能有很好的准确度但不能准确预测后续的数据。
5. *欠拟合*：对当前已有的训练集和测试集的预测效果都不好。
6. *交叉验证*：将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。用于防止模型过于复杂而引起的过拟合。
###线性回归的原理
1. 对于多元方程  ![Alt text](./1557666192502.png)
给定数据集  ![Alt text](./1557666435631.png)，其中![Alt text](./1557666454991.png)，![Alt text](./1557666470008.png)。m是样本数，d是属性维度。
线性回归试图学得 ![Alt text](./1557666532127.png)，使得![Alt text](./1557666558838.png)
2. 预测值和真实值之间都肯定存在差异\varepsilon，对于每个样本：
			![Alt text](./1557666756749.png)      （2）
	假设误差![Alt text](./1557666825199.png)是独立同分布的，并且服从高斯分布。即：
	![Alt text](./1557666840697.png)               （3）
将（2）代入（3）中，得到在已知参数w和数据w_i的情况下，预测值为y_i的条件概率：
   ![Alt text](./1557666882654.png)     （4）
3.  将（4）连乘得到在已知参数w和数据x的情况下，预测值为y的条件概率，这个条件概率在数值上等于，likelihood（w|x,y），也就是在已知现有数据的条件下，w是真正参数的概率，即似然函数（5）：
    对数似然函数：
    ![Alt text](./1557666961289.png)       （6）
 得到目标函数：
 ![Alt text](./1557666991018.png)      （7）
为什么要让目标函数越小越好：似然函数表示样本成为真实的概率，似然函数越大越好，也就是目标函数![Alt text](./1557667191845.png)
越小越好。
4. 目标函数是凸函数，只要找到一阶导数为0的位置，就找到了最优解。
因此求偏导：
![Alt text](./1557667043640.png)     （8）
5. 令偏导等于0
![Alt text](./1557667068533.png)  （9）
得到：![Alt text](./1557667085613.png)  （10）
情况一：{X^T}X可逆，唯一解。令公式（10）为零可得最优解为：
![Alt text](./1557667114517.png)    （11）
 学得的线性回归模型为:
 ![Alt text](./1557667126374.png)    （12）
情况二：{X^T}X不可逆，可能有多个解。选择哪一个解作为输出，将有学习算法的偏好决定，常见的做法是增加![Alt text](./1557667172247.png)扰动。
![Alt text](./1557667154527.png)   （13）
###线性回归损失函数、代价函数、目标函数
1. *损失函数*：定义在单个样本上的，算的是一个样本的误差。
2. *代价函数*：是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
3. *目标函数*：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。
###优化方法(梯度下降法、牛顿法、拟牛顿法等)
[梯度下降法、牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/37524275)
###线性回归的评估指标
1、 SSE(和方差)
该统计参数计算的是拟合数据和原始数据对应点的误差的平方和，计算公式如下
![Alt text](./1557668454696.png)
SSE越接近于0，说明模型选择和拟合更好，数据预测也越成功。接下来的MSE和RMSE因为和SSE是同出一宗，所以效果一样

2、 MSE(均方差)
该统计参数是预测数据和原始数据对应点误差的平方和的均值，也就是SSE/n，和SSE没有太大的区别，计算公式如下
![Alt text](./1557668464030.png)

3、 RMSE(均方根)
该统计参数，也叫回归系统的拟合标准差，是MSE的平方根，就算公式如下
![Alt text](./1557668471711.png)

在这之前，我们所有的误差参数都是基于预测值(y_hat)和原始值(y)之间的误差(即点对点)。从下面开始是所有的误差都是相对原始数据平均值(y_ba)而展开的(即点对全)!!!

4、R-square(确定系数)
在讲确定系数之前，我们需要介绍另外两个参数SSR和SST，因为确定系数就是由它们两个决定的
(1)SSR：Sumof squares of the regression，即预测数据与原始数据均值之差的平方和，公式如下
![Alt text](./1557668481421.png)
(2)SST：Totalsum of squares，即原始数据和均值之差的平方和，公式如下
![Alt text](./1557668488566.png)
SST=SSE+SSR，呵呵只是一个有趣的问题。而我们的“确定系数”是定义为SSR和SST的比值，故
![Alt text](./1557668522637.png)
其实“确定系数”是通过数据的变化来表征一个拟合的好坏。由上面的表达式可以知道“确定系数”的正常取值范围为[0 1]，越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好，R^2  越大，说明预测出来的数据可以通过模型的解释性就越强。
###sklearn参数详解
[sklearn中文手册](http://sklearn.apachecn.org/#/)
