{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I0sx60GoFwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "# __author__ = 'Administrator'\n",
        " \n",
        "import numpy as np\n",
        " \n",
        "def loadDataSet():#数据格式\n",
        "    postingList = []\n",
        "    for line in open(\"/data.txt\"):\n",
        "        postingList.append(line.split())\n",
        "    print(postingList)\n",
        " \n",
        "    '''postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
        "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
        "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
        "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
        "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
        "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]'''\n",
        " \n",
        "    classVec = [0,1,0,1,0,1]#1 侮辱性文字 ， 0 代表正常言论\n",
        "    return postingList,classVec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nufk_K_boMO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def createVocabList(dataSet):#创建词汇表\n",
        "    vocabSet = set([])\n",
        "    for document in dataSet:\n",
        "        vocabSet = vocabSet | set(document) #创建并集\n",
        "    return list(vocabSet)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh0zdlRWoOJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def bagOfWord2VecMN(vocabList,inputSet):#根据词汇表，讲句子转化为向量\n",
        "    returnVec = [0]*len(vocabList)\n",
        "    for word in inputSet:\n",
        "        if word in vocabList:\n",
        "            returnVec[vocabList.index(word)] += 1\n",
        "    return returnVec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jp8JGg3oQLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def trainNB0(trainMatrix,trainCategory):\n",
        "    numTrainDocs = len(trainMatrix)  #向量的个数\n",
        "    numWords = len(trainMatrix[0])   #总共32个词\n",
        "    pAbusive = sum(trainCategory)/float(numTrainDocs) #Category = 1 所占比例（先验概率）\n",
        "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)#计算频数初始化为1\n",
        "    p0Denom = 2.0; p1Denom = 2.0                  #即拉普拉斯平滑\n",
        "    for i in range(numTrainDocs):\n",
        "        if trainCategory[i] == 1:\n",
        "            p1Num += trainMatrix[i]\n",
        "            p1Denom += sum(trainMatrix[i])\n",
        "        else:\n",
        "            p0Num += trainMatrix[i]\n",
        "            p0Denom += sum(trainMatrix[i])\n",
        "    p1Vect = np.log(p1Num/p1Denom)#注意  计算单词出现的概率 分母是这一类所有词的个数\n",
        "    p0Vect = np.log(p0Num/p0Denom)#注意\n",
        "    return p0Vect,p1Vect,pAbusive#返回各类对应特征的条件概率向量\n",
        "                                 #和各类的先验概率\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lUSGjWUoSy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
        "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)#注意\n",
        "    p0 = sum(vec2Classify * p0Vec) + np.log(1-pClass1)#注意\n",
        "    if p1 > p0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3wVFUaMoUE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def testingNB():#流程展示\n",
        "    listOPosts,listClasses = loadDataSet()#加载数据\n",
        "    myVocabList = createVocabList(listOPosts)#建立词汇表\n",
        "    trainMat = []\n",
        "    for postinDoc in listOPosts:\n",
        "        trainMat.append(bagOfWord2VecMN(myVocabList,postinDoc))\n",
        "    p0V,p1V,pAb = trainNB0(trainMat,listClasses)#训练\n",
        "    #测试\n",
        "    #testEntry = ['love','my','dalmation','dog','stupid']\n",
        "    testEntry = ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
        "    thisDoc = bagOfWord2VecMN(myVocabList,testEntry)\n",
        "    print( testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZqWZqE1oVye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1a8d89ec-0db2-4c31-8526-b1a659cc5561"
      },
      "source": [
        "testingNB()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
            "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'] classified as:  1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}